\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}		% for \lstlisting
\usepackage[left=3cm,right=3cm,top=3cm,bottom=3cm]{geometry}
\usepackage{hyperref}		% for clickeable table of contents
\usepackage{subcaption}		% for subfigures


\newcommand{\pgl}[1]{\textsf{#1}}
\newcommand{\perltoc}[2]{
Perl:
\begin{lstlisting}[language=perl]
aa
\end{lstlisting}

C:
\begin{lstlisting}[language=C]
bb
\end{lstlisting}
}

\newcommand{\bash}[1]{\texttt{#1}}
\newcommand{\regexp}[1]{$#1$}
\newcommand{\ccode}[1]{\texttt{#1}}
\newcommand{\haskell}[1]{\textsf{#1}}

\newcommand{\insertfa}[3]{
\begin{figure}[h!]
	\centering
	\includegraphics[#3]{#1}
	\caption{#2}
\end{figure}
}

\newcommand{\smallnfa}[2]{\insertfa{./img/nfa/#1.png}{NFA for #2}{scale=0.7}}
\newcommand{\largenfa}[2]{\insertfa{./img/nfa/#1.png}{NFA for #2}{width=\textwidth}}
\newcommand{\smalldfa}[2]{\insertfa{./img/dfa/#1.png}{DFA for #2}{scale=0.7}}
\newcommand{\largedfa}[2]{\insertfa{./img/dfa/#1.png}{DFA for #2}{width=\textwidth}}

\newcommand{\nfatodfa}[4]{
\begin{figure}[h]
	\begin{subfigure}[h]{0.5\textwidth}
	\centering
	\includegraphics[#3]{./img/nfa/#1.png}
	\caption{NFA for #2}
	\end{subfigure}
	~
	\begin{subfigure}[h]{0.5\textwidth}
	\centering
	\includegraphics[#4]{./img/dfa/#1.png}
	\caption{DFA for #2}
	\end{subfigure}
	\caption{}
\end{figure}
}

\lstdefinestyle{C}{
	language=C,
	tabsize=4,
	frame=L
}


\title{Conq: A Perl to C compiler}
\author{Hugo Kapp}

\begin{document}

\maketitle

% Here goes the text for the preface

\tableofcontents

\chapter*{Introduction}

Perl is a famous scripting language, mostly used for text handling. It provides regular expressions as part of its core elements, and permits easy rewriting of text files, loading of config files, csv building / extraction ...

Perl is an interpreted language. The main reason for this is the fact that it is a scripting language in the pure tradition of \pgl{sh} or \pgl{python}. This means that it is dynamically typed, and runtime checks must be performed to figure out the semantics of every basic operator. This is easier done in an interpreter than in machine code.

Having an interpreter has the disadvantage of being slow when executing regular expression matching. This gives poor performance for regular expression heavy programs, which is supposed to be the core of Perl.

%This implies poor performance for programs that don't use variables, or in which variables never change type. To this end, we want to provide an alternative to the classic Perl interpreter for these use cases.

We describe an alternative, called Conq. Conq is a Perl to C compiler. This means we can compile Perl programs down to machine code, improving the performance of regular expression matching. Later, we may also investigate runtime specialization to address the performance issues related to runtime type dispatch.

\chapter{Description of the project}

\section{Efficient regular expression matching}

The main point we want to address is the translation from regular expression to efficient machine code. We value this aspect for the following reasons :
\begin{itemize}
\item This is the part where we believe we can achieve the biggest improvement over the current Perl interpreter
\item This is the most interesting part to work on
\end{itemize}

\section{Runtime specialization}

An orthogonal point we could investigate is linked to runtime specialization. There are indeed a lot of room for optimization with runtime specialization in Perl scripts.

One case of such optimizations opportunities is regarding variables and operator typing. Due to the dynamically-typed nature of Perl, each operator must, at runtime, check which types its operand are, and apply the correct semantics with regard to this type. This means that, if $a = a + b$ must be executed, then in the case where $a$ and $b$ are both integers the program would perform addition, but if $a$ and $b$ are strings, then the operation is a concatenation. This typing information is only available at runtime.

Lots of things can be done in the case described above. First, we can profile the input types to this $+$ operator. If they are always the same, we can replace the operator runtime checks by the actual code for the correct semantics.

We can also profile the types of the variables $a$ and $b$. If they never change, we can actually replace the code of every operator using $a$ and $b$.

Another place for runtime specialization in Perl scripts comes from dynamic regular expressions. Consider the following program : $/aab\$d/$. Different scenarios may arise from the previous example :
\begin{enumerate}
\item $d$ is a program constant \\
The regular expression can be compiled down to machine code\\
This can be done with static analysis
\item $d$ is a per-execution constant \\
The regular expression can be compiled down to machine code for each execution of the program (only useful if executed more than once)\\
This can be found out statically, but the code must be generated at runtime
\item $d$ is not constant, but always evaluates to the same value at runtime \\
This can only be found out at runtime using profiling \\
Machine code (with guards) can be generated at runtime
\item $d$ may have multiple values at runtime \\
Nothing can be done. A generic and inefficient version must be generated ahead of time
\end{enumerate}

Case 1 can be done completely ahead of time. Case 2 must generate the code at runtime, but the information can already be given ahead of time. Case 3 must be done at runtime (not guaranteed to yield results though).

The lifespan of these specializations is not determined yet. It may either be :
\begin{itemize}
\item Per execution \\
This has the advantage of reflecting the types of the currently executing program, but must be reexecuted everytime and makes the specialization process hard to write (code rewriting during execution)
\item Per script \\
In this variant, runtime information is gathered during execution, and specialized code generated at the end of the execution, then recompiled\\
This makes the specialization process easier to write, and allows subsequent executions of the same scripts to be faster right away.
\end{itemize}

Because it is simpler to write and makes a lot of sense (we believe variables used by the programmer always have the same meaning between executions, i.e. always the same type), we might rather go for the second option.

Generally, we consider runtime specialization to be a late optimization in the development process.

\section{Provide a viable replacement for Perl}

Eventually, this means providing all the options \pgl{Perl} does. The most important here might be the in-line script definition, i.e. \bash{perl -e 's/aa/a/g'}.

One other important aspect of \pgl{Perl} we must provide is the reading from files (also from stdin).

\section{Implementation language}

The idea is to implement this compiler completely in \pgl{Haskell}. This will be a good exercise to learn this functional language.

Furthermore, representing the IR with immutable data structures makes a lot of sense. Finally, pattern matching is real gift in compiler construction.

All in all, functional languages are really well-suited to write compilers, and this will make me learn \pgl{Haskell}.

\section{Roadmap for language support}

The roadmap for language support should be the following :
\begin{enumerate}
\item Regular expressions
	\begin{enumerate}
	\item Simple regular expression matching
	\item Static replacements
	\item Complete regular expression matching
	\item Dynamic replacements	
	\end{enumerate}
\item Scripting
	\begin{enumerate}
	\item Global variables
	\item Variables assigned regular expression results
	\item Printing
	\item Basic operators
	\item Control flow
	\item stdin handling
	\item File handling
	\end{enumerate}
\end{enumerate}

\chapter{Regular expressions to DFA}

There is already theory about how to represent a regular expression as a DFA. It is easier to turn the regexp into a NFA, and then use basic rules to transform the latter into a DFA.

\section{Regular expressions to NFA}

\label{regexptonfa}

We provide here a recap of how the various elements of a regular expression can be transformed into an NFA.

In regular expressions, small letters $a$, $b$, ... are single letters in the regular expressions, while CAPS letters $A$, $B$, ... represent sub-regular expression.

\subsubsection{Concatenation}

\smallnfa{concat}{\regexp{AB}}

\subsubsection{Alternative}

\smallnfa{alt}{\regexp{A|B}}

\subsubsection{Option}

\smallnfa{option}{\regexp{A?}}

\subsubsection{Negation}

\smallnfa{neg}{\regexp{[\ \hat{} A]}}

\subsubsection{Repetitions}

There are multiple kinds of repetition, though we can desugar most of them into simpler elements.

The basic repetition, the $'*'$ operator, also called the \textit{Kleene star}, accepts any number of repetitions of the subexpression. 0 repetitions is accepted by Kleene star.

\smallnfa{kleene-star}{\regexp{A*}}

All other repetition operators can be expressed using Kleene star, which makes it the only required repetition operator.

The $'+'$ repetition operator requires at least one repetition of the given subexpression. \regexp{A+} can be rewritten as \regexp{AA*}.

\smallnfa{repeat-at-least-once}{\regexp{A+}. This is equivalent to \regexp{AA*}}

The "repeat n times" operator has the following syntax : \regexp{A\{n\}}. This can be rewritten as n concatenations of $A$, e.g. \regexp{A\{n\} = AA..A\ (n\ times)}.

\smallnfa{repeat-n-times}{\regexp{A\{n\}}. This is equivalent to \regexp{AA..A\ (n\ times)}}

Note however that we might prefer to do a C-loop in this case rather than copying n times the code, to avoid code explosion. Copying it would then be a loop unrolling optimization, which can be decided based on other information.

The "repeat at least n times" operator is written \regexp{A\{n,\}}. It can be rewritten using \regexp{A\{n\}} and Kleene star : \regexp{A\{n,\} = A\{n\}A* = AA..AA*}.

Note that \regexp{A\{1,\} = A+}. This can easily be seen by comparing the rewritten expression for both operators.

\largenfa{repeat-at-least-n-times}{\regexp{A\{n,\}}. This is equivalent to \regexp{A\{n\}A*}}

The "repeat at most m times" operator is the complement of the previous, and is written as \regexp{A\{,m\}}. It can be rewritten as \regexp{A?A?...A?\ (m\ times)}.

\largenfa{repeat-at-most-m-times}{\regexp{A\{,m\}}. This is equivalent to \regexp{A?A?...A?\ (m\ times)}}

Same comment here about loop unrolling as made regarding \regexp{A\{n\}}. Note also that \regexp{A\{,1\} = A?}

The last repetition operator puts lower and upper bounds on the number of repetitions. "Repeat between n and m times" is written as \regexp{A\{n, m\}}. As one can guess from the syntax and semantics, \regexp{A\{n, m\}} is rewritten using \regexp{A\{n\}} and \regexp{A\{, m\}} : \regexp{A\{n, m\} = A\{n\}A\{, m-n\}}. The (cumbersome) rewriting into only basic elements is \regexp{A\{n, m\} = AA..AA?A?..A?}

\largenfa{repeat-between-n-and-m-times}{\regexp{A\{n,m\}}. This is equivalent to \regexp{A\{n\}A\{,m-n\}}}

With correct rewriting, only the Kleene star and the "repeat n times" operators must be implemented to support all the repetition operators.

\section{From NFA to DFA}
\label{nfatodfa}

We provide here the simple rules that can be used to turn an NFA into a DFA.

Starting from the NFAs listed in Section \ref{regexptonfa}, we only need to describe a way to translate choice-based patterns from NFAs to DFAs. This yields a small number of patterns :
\begin{itemize}
\item Choice between $A$ and $B$
\item Choice between $A$ and the accepting state
\end{itemize}

To be able to give a translation for these choice-based patterns, we must provide an ordering for the different options. For subexpressions, the ordering is given in the original program : the left-most variant has precedence over the ones on the right. Then, following the "longest match" semantics of regular expressions, subexpressions have precedence over the accepting state.

This is enough to translate all the NFA patterns given above.

\nfatodfa{alt}{\regexp{A|B}}{width=\textwidth}{width=\textwidth}
\nfatodfa{option}{\regexp{A?}}{width=\textwidth}{scale=0.7}
\nfatodfa{kleene-star}{\regexp{A*}}{scale=0.7}{scale=0.7}


\section{From regular expression directly to DFA}

\label{regexptodfa}

Using the rules defined in the sections above, we give here a direct translation from the basic elements of regular expressions into a DFA. This is what is used in our system.

This section is mostly useful for a quick lookup during the implementation phase, as it is not adding any information (only combining the patterns from Sections \ref{regexptonfa} and \ref{nfatodfa}).

% Basic regexes
\smalldfa{concat}{\regexp{AB}}

\smalldfa{alt}{\regexp{A|B}}

\smalldfa{option}{\regexp{A?}}

\smalldfa{neg}{\regexp{[\ \hat{} A]}}

% Repetitions

\smalldfa{kleene-star}{\regexp{A*}}

\smalldfa{repeat-at-least-once}{\regexp{A+}}

\smalldfa{repeat-n-times}{\regexp{A\{n\}}}

\smalldfa{repeat-at-least-n-times}{\regexp{A\{n,\}}}

\smalldfa{repeat-at-most-m-times}{\regexp{A\{,m\}}}

\largedfa{repeat-between-n-and-m-times}{\regexp{A\{n,m\}}}

\chapter{Regular expressions compilation}

We lay out in this section the design for the compilation of regular expressions.

\section{Parsing}

\section{Representating a DFA}

\subsection{The essence of a DFA}

To figure out how to store and represent DFAs inside the system, we need to get to the essentials of what a DFA is.

A DFA is defined by a set of states and transitions. It has exactly one starting state, and at least one accepting and one rejecting state. These multiplicities can trivially be turned into exactly one accepting and one rejecting state, which is what we are going to use when representing DFAs.

The accepting and rejecting state are \textit{final states}, i.e. no transition is possible from these states. Every other state in the DFA must have at least one transition, and there must be a path from every state to a final state. When a state has more than one transition, the input decides which transition is taken. An input that matches no transition in the current state means going to the rejecting state. This is usually implicit.

Note that a final state may have no incoming transitions. This is the case in \regexp{A?}, where the DFA always ends in the accepting state.

This description of a DFA is very similar to a directed graph, with additional constraints.

\subsection{Requirements}

From the essence of a DFA discussed above, we can derive the requirements in terms of data and constraints our representation must fulfill :
\begin{itemize}
\item States must be identifiable
\item Transition must know their valid input (i.e. the input leading to this transition)
\item We must know which are the starting, accepting and rejecting states
	\begin{itemize}
	\item There should be exactly one of each
	\item Final states not reachable could be removed
	\end{itemize}
\end{itemize}

When constructing regexes from subexpressions:
\begin{itemize}
\item We must be able to re-wire the rejecting state
	\begin{itemize}
	\item This certainly means making the "unmatched" path in every state explicit
	\item This can be treated specifically or generically (i.e. treating the rejecting state as a normal one)
	\end{itemize}
\item We must remember the shape of each DFA, i.e. starting and ending boundaries
	\begin{itemize}
	\item This is mainly for state restoration
	\end{itemize}
\end{itemize}

\subsection{Intermediate representation}

We use only the abstraction of a state. A state is comprised of :
\begin{itemize}
\item A unique reference (a string or numeric value)
\item A set of transitions
	\begin{itemize}
	\item Multiple "input string to destination state" transitions\\
		The destination state is identified by its reference
	\item A special "else" transition
	\end{itemize}
\end{itemize}

We may simplify the model of a state to accept only a single string. The multiple transitions version is only an optimization. Note that a list there also allows for no transitions at all (is that useful ?).

A DFA is only a map of references to states. It also needs a reference to its starting, accepting and rejecting states.

On top of the basic input-reading states, we add other states for specific tasks. These are used to represent states that have a special meaning in our system, but no relevance in a classic DFA. This is discussed in more details in Section \ref{specialstates}.

The first special states we need to add are a global accepting and rejecting state. These states do not have any outgoing state transition.

To solve the problem of backtracking and \regexp{A\{n\}}, we propose to add special states that represent some code generation / generic computation in the program. For example, a "pass n times" state that increases a counter every time it gets activated and goes to the accepting state once its counter reaches $n$. Also, a pair save / restore states for backtracking.


\subsection{How to build regular expressions from subexpressions}

To perform rewiring, we replace the entries for the accepting and rejecting state of the original DFA with a special \textsf{goto} state. The latter points to the next state via its reference. This operation is $O(1)$.

An example of how this works for \regexp{AB} and \regexp{A|B} is given in Figures \ref{concatir} and \ref{altir}.

This solution requires us to construct state references unique among the whole system. This can be fixed by having a global counter (\textit{though this is not very Haskell-y}).


\begin{figure}[h]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./img/dfa/concat.png}
		\caption{DFA for \regexp{AB}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
concat(A, B) {
	AB.states = A.states ++ B.states;
	AB.states(A.accept) = goto(B.start);
	AB.states(A.reject) = goto(B.reject);
	AB.start = A.start;
	AB.accept = B.accept;
	AB.reject = B.reject;
}
		\end{lstlisting}
		\caption{Haskell code for building \regexp{AB}}
	\end{subfigure}
	\caption{}
	\label{concatir}
\end{figure}

\begin{figure}[h]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./img/dfa/alt.png}
		\caption{DFA for \regexp{A|B}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
alt(A, B) {
	AorB.states = A.states ++ B.states;
	saveID = nextID(..);
	AorB(saveID) = 
		saveState(then => A.start);
	AorB.states(A.accept) = goto(B.accept);
	AorB.states(A.reject) = 
		restoreState(then => B.start);
	AorB.start = saveID;
	AorB.accept = B.accept;
	AorB.reject = B.reject;
}
		\end{lstlisting}
		\caption{Haskell code for building \regexp{A|B}}
	\end{subfigure}
	
	\centering
	\begin{subfigure}[h]{0.85\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./img/ir/alt.png}
		\caption{Abstract representation of \regexp{A|B}}
	\end{subfigure}
	\caption{Intermediate representation for \regexp{A|B}}
	\label{altir}
\end{figure}

\subsection{Advantages of this representation}

This representation of a DFA has the advantage of being flat. There is no wrapping, sub-states, contexts or syntax trees. Every state lives in the same subexpression in the end. This is much simpler to manipulate.

The data structures used to represent a DFA are also as simple as possible : mainly a map from the references to the states.

The fact that we can introduce any other state in the representation is a good point. It means we can add states that are not real DFA states, but added information in the lowering phases.

An example of that would be for backtracking (see \ref{backtracking}). At AST build time, we could add the relevant \haskell{save} and \haskell{restore} states. This will ease code generation, and allow us to build a classic lowering compiler.

\subsection{Drawbacks of this representation}

This model introduces "added" or "garbage" state, typically the \haskell{goto} state. Although this makes rewiring simpler, it also means we need to add a pass removing these states and changing the destination reference in the states themselves.

When going from regular expression to this pure DFA representation, we lose all information linked to the regexp operators used in the original program. We do not know anymore which are used, which subset of states belong to which operator (if any), etc... This might make some optimizations much harder / borderline impossible.

Also, lowering passes that require this information must either be done at DFA creation time, or using heavy graph analysis (e.g. backtracking).

Generally speaking, analysis on the programs will be tricky, because it needs to be graph analysis. Optimizing this structure will be very hard. We can alleviate this by focusing our work on graph theory and known DFA results.

\subsection{Alternative representation: classic AST}

Alternatively to the representation described above, we could use a classic AST representation, with regex operators as nodes, to represent regular expressions.

This representation could be used as the result of regex parsing, and then be turned into the DFA representation later (as a lowering pass).

We do not envision to start with this representation for a first version. However, it could be a solution to add optimizations that are too hard to implement with the pure DFA / state graph representation.

\section{Elements not captured by the DFA}

\label{specialstates}

We discuss in this section elements that are required for code generation, or come from Perl regular expression specification, but do not appear in the corresponding DFA. The solution for most of these issues will be to add a new special state in the graph.

\subsection{Avoid DFA copies}

We show here how to avoid copying the same DFA multiple times when representing the repetition operators \regexp{A\{n\}}, \regexp{A\{n,\}}, \regexp{A\{,m\}} and \regexp{A\{n,m\}}.

First, we remind to the reader that these two operators do not require a specific DFA construct. They are equivalent to the following rewriting :
\begin{align*}
A\{n\} &= AA..A &(n\ times) \\
A\{n,\} &= A\{n\}A*  \\
A\{,m\} &= A?A?..A? &(m\ times) \\
A\{n,m\} &= A\{n\}A\{,m-n\}
\end{align*}

However, $A$ may be an arbitrarily complex DFA, with itself repetition operators inside. This woud lead to a great number of state, and large amounts of generated code, for operations that are equivalent.

We want to be able to represent these operators without copying the DFA for $A$, and without emitting the same code multiple times.

Before going further, note that we can rewrite \regexp{A\{n\}}, \regexp{A\{n,\}}  and \regexp{A\{,m\}}  using \regexp{A\{n,m\}}:
\begin{align*}
A\{n\} &= A\{n,n\} \\
A\{n,\} &= A\{n,n\}A* \\
A\{,m\} &= A\{0,m\} \\
A\{n,m\} &= AA..AA?A?..A?
\end{align*}

Therefore, providing a solution for \regexp{A\{n,m\}} is enough to solve the whole issue.

For this, we introduce two new states :
\begin{itemize}
\item \haskell{repeat\_acc} when the repeated subexpression matches
\item \haskell{repeat\_rej} when the repeated subexpression does not match
\end{itemize}

Each of these two states must be initialized with the constants $n$ and $m$. They will also share a variable we call \haskell{cnt}, initialized to $0$. The subexpression $A$ must be rewired accordingly. A graphical representation of this addition is shown in Figure \ref{repeatnmir}.

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{./img/ir/repeatnm.png}
	\caption{Abstract representation of \regexp{A\{n,m\}}}
	\label{repeatnmir}
\end{figure}

The semantic for \haskell{repeat\_acc} is the following :
\begin{enumerate}
\item Increment \haskell{cnt} by one
\item If $\haskell{cnt} < m$ then loop back to $A$
\item If $\haskell{cnt} = m$ then accept 
\end{enumerate}

The semantic for \haskell{repeat\_rej} is the following :
\begin{enumerate}
\item If $\haskell{cnt} < n$ then reject
\item If $\haskell{cnt} \geq n$ then accept
\end{enumerate}

Because \regexp{A\{n,m\}} may also be used inside a repetition operator itself, the variable \haskell{cnt} must be reset when the whole pattern accepts or rejects.

\subsection{Backtracking}

\label{backtracking}

As a subexpression matches the input stream, it will modify the read position. Once it is over, this value becomes the starting position for the next subexpression. This means that subexpression $B$ reads the remaining input, i.e. what $A$ hasn't read yet.

If subexpressions $A$ and $B$ have been rewired such that $B$ starts its match when $A$ rejects (e.g. \regexp{A|B}), then $B$ should start reading from the point where $A$ started its match. This means that $A$ should remember its input position, and restore this value when reaching the rejecting state.

Note that storing and restoring input values is not necessary for every subexpression in the program. This is only required in patterns where the rejecting state of a subexpression is used to start another subexpression. \textit{In fine}, this means we do not need to generate the store-and-restore code for every subexpression, but only in patterns that need it.

To explicit this behavior, we add a \haskell{savePos} and a \haskell{restorePos} state to the DFA. These two states share a variable called \haskell{saved\_pos}. We show how these states are used in Figure \ref{backtrackexample}.

\begin{figure}[t]
	\centering
	\includegraphics[scale=0.7]{./img/ir/backtrack.png}
	\caption{Backtracking example}
	\label{backtrackexample}
\end{figure}

\subsection{Backtracking on streams}

If the input behaves like a stream (only possible to consume characters, but not possible to go back), then caching the stream input value must be performed whenever we need to backtrack.

\subsection{Filling regexp variables}
\label{regexpvariables}

Regular expressions define variables that represent a subset of the matched string. We describe here how these are managed in Conq.

Each Perl regular expression defines between 1 and 11 variables that can be used after the match has been performed.

The full string matched by the regular expression is always stored in \regexp{\$0}. Then, for every subexpression enclosed in parenthesis, the string matched is stored in a variable from \regexp{\$1} to \regexp{9}. Perl doesn't allow more submatches to be stored.

The first thing to note here is that regular expressions, before starting their match, should always clear the previously stored results.

Then, if the input string behaves like a stream, we have to store at least the part that matches. Every character read from the stream must be copied into a shared variable. This being computation-intensive, we look at optimizing this process in sections \ref{regexpvariablesoptimization} and \ref{streamcachingopt}.

Depending on how we represent strings at runtime (see Section \ref{stringrep}), we may simply use sub-indices on a buffer to represent and fill the regexp variables. Otherwise, we may have to copy the relevant substring.

\section{Code generation}

We explain in this section the basis for generating C code from the input DFA.

\subsection{Regular expressions and subexpressions}

In the generated C code, a regular expression is a continuous sequence of instructions (i.e. not a function). Each subexpression bases its computation on the following elements :
\begin{itemize}
\item the global variable \ccode{input} of type \ccode{char*}, representing the input stream
\item the global variable \ccode{pos}, specifying where to start match this subpattern
\end{itemize}

Ending of the input stream is not clearly defined yet. Either it should be the \texttt{EOF} character, which shouldn't match any character of any pattern, or it should be an added \ccode{length} or \ccode{lastidx} against which stream accesses should be tested. The latter seems less performant.

Once the matching of a subexpression is over, it either falls into the accepting state or the rejecting state. We model these two states as two C labels. These can then be used to assemble subexpressions.

With this approach, translation from any DFA to C is trivial.

\section{DFA translation}

For each DFA in \ref{regexptodfa}, we give here its translation in C.

\begin{figure}[h!]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./img/dfa/concat.png}
		\caption{DFA for \regexp{AB}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
label:start
// match A

label:accept_A
	goto start_B;

label:reject_A
	goto reject;

label:start_B
// match B

label:accept_B
	goto accept;

label:reject_B
	goto:reject;

label:accept
label:reject
		\end{lstlisting}
		\caption{C code for \regexp{AB}}
	\end{subfigure}
	\caption{}
\end{figure}

\begin{figure}[h!]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./img/dfa/alt.png}
		\caption{DFA for \regexp{A|B}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
label:start
int backup = pos;
// match A

label:accept_A
	goto accept;

label:reject_A
	pos = backup;
	goto start_B;

label:start_B
// match B

label:accept_B
	goto accept;

label:reject_B
	goto:reject;

label:accept
label:reject
		\end{lstlisting}
		\caption{C code for \regexp{A|B}}
	\end{subfigure}
	\caption{}
	\label{dfatoc-alt}
\end{figure}

\begin{figure}[h!]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[scale=0.7]{./img/dfa/option.png}
		\caption{DFA for \regexp{A?}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
label:start
int backup = pos;
// match A

label:accept_A
	goto accept;

label:reject_A
	pos = backup;
	goto accept;

label:accept
		\end{lstlisting}
		\caption{C code for \regexp{A?}}
	\end{subfigure}
	\caption{}
\end{figure}

\begin{figure}[h!]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[scale=0.7]{./img/dfa/neg.png}
		\caption{DFA for \regexp{[\ \hat{} A]}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
label:start
int backup = pos;
// match A

label:accept_A
	goto reject;

label:reject_A
	pos = backup;
	goto accept;

label:accept
label:reject
		\end{lstlisting}
		\caption{C code for \regexp{[\ \hat{} A]}}
	\end{subfigure}
	\caption{}
\end{figure}

\begin{figure}[h!]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[scale=0.7]{./img/dfa/kleene-star.png}
		\caption{DFA for \regexp{A*}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
label:start
int backup = pos;
// match A

label:accept_A
	backup = pos;
	goto start_A;

label:reject_A
	pos = backup;
	goto accept;

label:accept
		\end{lstlisting}
		\caption{C code for \regexp{A*}}
	\end{subfigure}
	\caption{}
\end{figure}

\begin{figure}[h!]
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{./img/dfa/repeat-n-times.png}
		\caption{DFA for \regexp{A\{n\}}}
	\end{subfigure}
	\hspace{0.05\textwidth}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\begin{lstlisting}[style=C]
label:start
for (i = 0; i < n ; i++) {
	// match A

label:accept_A
	// do nothing

label:reject_A
	goto reject;
}

goto accept;

label:accept
label:reject
		\end{lstlisting}
		\caption{C code for \regexp{A\{n\}}}
	\end{subfigure}
	\caption{}
	\label{dfatoc-ntimes}
\end{figure}

\chapter{Scripts compilation}

\section{Intermediate Representation}
\label{ir}

We detail here the representation we use for DFAs inside the system, i.e. our IR.

We need two IRs for this compiler. As a general statement, we are going to copy what is done in ScalaNative for this.

The first IR should represent regular expressions using a DFA.

The second IR should lower the DFA representation into a more control-flow oriented one, most likely block based with a label and a control-flow op in each.

Note that the scripting part needs neither. Indeed, its representation is already procedural, so DFA IR does not work. But a control-flow / block based representation does not work either,  because it may have non-jump-based control flow (like for or while loops, which we would still like to keep in our IR and generated code). A full-on AST should be fine here.

Note that the second IR is not able to represent the generated code shown in \ref{dfatoc-ntimes}.

For control-flow optimization on regexp code, we could represent it in a procedural fashion, then "re-discover" the CFG. Note that with an added label "loop-end", the generated code shown in \ref{dfatoc-ntimes} can be represented using block-based IR.

If we merge the two representations, then we are able to jump directly from the regexp rejecting an input to the corresponding script code. Finding a suitable IR for both is then very important. It also avoids the need to rewrite every optimization twice.

With block-based IR, sequence of instructions that go to a regular control-flow operation can still be represented with a goto on a temporary label. These labels need not be generated, at least not if there is a single input to it.

Note that if we do the whole control-flow ourselves with block-based approach, then the translation to C will be rather poor. We should maybe go to LLVM then. Pure AST form is simpler to translate to C.

More work has to be done here.

Current best solution : represent both regexes and perl scripting code in IR form, and perform control-flow based optimizations on both at the same time, generating the CFG from the IR form.

\chapter{Runtime management}

\section{Strings and memory management}
\label{stringrep}

In this section :
\begin{itemize}
\item How we represent strings at runtime
\item Our model for strings (most likely immutable)
\item Description of the memory management with an emphasis on strings
\end{itemize}

\chapter{Optimizations}

\section{Avoiding filling regexp variables}
\label{regexpvariablesoptimization}

With static analysis, we can easily figure out if the value of a \textit{regexp variable} (\ref{regexpvariables}) is used or not before the next pattern match. In this case, we can remove the whole code that deals with filling these variables.

Note that this is a specific case of dead-code elimination. It could be able to figure this out completely, but the complex CFG might throw the optimizer off.

A similar optimization can be done to avoid clearing a regexp variable in the following cases :
\begin{itemize}
\item the variable will be overwritten to by this regular expression\\
This has to be true on every control flow path
\item the variable has not been written to by any previous regular expression
\item the variable is not used in the program before the next regular expression\\
This can be done at the same time we do regexp variable filling elimination
\end{itemize}

\section{Optimizing stream caching}
\label{streamcachingopt}

\subsection{Batch read}

In the case where we need to cache a stream value (and maybe even if we just need to read it), we may want to read the stream in batch mode.

In this case, when there are no new elements to read in the stream cache, the program must call a function to do the following :
\begin{enumerate}
\item On first call, allocate a string of size 10, then read 10 \ccode{char} from the stream into it
\item On second call, reallocate the string to size 110. Read the next 100 characters into it
\item On following calls, extend the string by 1000 and read the next 1000 characters into it. This is 1kB of data, which will take some time to process in the regular expression
\end{enumerate}

The stream elements read too many must be kept and supplied to the next stream access. Otherwise, some elements would be missing.

\subsection{Cache sharing}

We may share the cache read so far with other parts of the program. Use cases where this happens are obscure so far.

\section{Simple runtime specialization}


Perl:
\begin{lstlisting}[language=perl]
a = a + b;
\end{lstlisting}

C:
\begin{lstlisting}[language=C]
lasttypea;
lasttypeb;
if (a.type == lasttypea && b.type == lasttypeb)
  goto lastjump
else if (a.type == int && b.type == int) {
:plusint
  a = a + b;
  lasttypea=int;
  lasttypeb=int;
  lastjump = plusint;
}
else if (...) {	
...
\end{lstlisting}

\section{Typed operation dispatch}


Perl:
\begin{lstlisting}[language=perl]
a = a + b;
\end{lstlisting}

C:
\begin{lstlisting}[language=C]
pluslabels gotolabels[][] = ...;
goto pluslabels[a.typecode][b.typecode];
\end{lstlisting}

\chapter{Implementation}

\section{Testing}

Due to the fact that Conq is supposed to be a transparent replacement for \pgl{Perl}, we can simply test our software by using both commands on the same input, with the same options, and check that the output produced is the same.

We will also need functional testing, i.e. tests for individual functions. This is very \pgl{Haskell}-esque.

\chapter{Discussion points}

\section{Source and destination language}

\subsection{Source language}

We have two options here :
\begin{enumerate}
\item Start from source Perl scripts
\item Compile the already anayzed Perl bytecode
\end{enumerate}

\begin{tabular}{|r|l|l|}
\hline
	& Perl scripts	& Perl bytecode \\
\hline
Advantages	& All information about the program	& Already compiled and analyzed source code \\
	&	& Can be integrated in the current Perl interpreter as a runtime compiler \\
\hline
Disadvantages	& Complex parser	& No need to reimplement the parser \\
\hline
\end{tabular}

\subsection{Destination language}

The destination language will mostly depend on the goals pursued. It can be either :
\begin{itemize}
\item A low-level language easily compiled down to machine code (most likely \pgl{C}, could be \pgl{LLVM})
\item Truffle
\end{itemize}

If we want to support a lot of runtime specializations, then it may be easier to go for Truffle (though we might loose on the regular expression part).

On the other hand, writing the runtime specializations by hand is a great learning exercise.

Furthermore, C might be a good destination language for a first working prototype, as it can very easily be generated from the Perl script AST. But if we want to do more heavy optimizations, and handle the control flow ourselves, then we might want to go to LLVM instead. Note that this will depend (and also have consequences) on the IR we use (see \ref{ir}).

\section{Dynamic regular expressions}

\end{document}